[
  {
    "objectID": "hands_on_machine_learning.html",
    "href": "hands_on_machine_learning.html",
    "title": "Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow",
    "section": "",
    "text": "Load packages\n\nimport os\nfrom pathlib import Path\nimport tarfile\nimport urllib.request\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n\n\nCode to save and format figures as high-res PNGs\n\nIMAGES_PATH = Path() / \"images\" / \"end_to_end_project\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\n# matplotlib settings\nplt.rc(\"font\", size=14)\nplt.rc(\"axes\", labelsize=14, titlesize=14)\nplt.rc(\"legend\", fontsize=14)\nplt.rc(\"xtick\", labelsize=10)\nplt.rc(\"ytick\", labelsize=10)\n\n\n\n\nSet seed for reproducbility\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED) # set seed for reproducibility\n\nAdditionally, you must set the environment variable PYTHONHASHSEED to \"0\" before python starts.\n\n\n\n\nExtract and load housing data\n\ndef extract_housing_data():\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True) # create new dir if does not exist\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n        with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path = \"datasets\")\n\n\ndef load_housing_data():\n    housing_csv_path = Path(\"datasets/housing/housing.csv\")\n    if not housing_csv_path.is_file():\n        extract_housing_data()\n    return pd.read_csv(housing_csv_path)\n\n\nhousing = load_housing_data()\n\nView housing data structure\n\nhousing.head()\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\n\nhousing.hist(bins=50, figsize=(12,8))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef shuffle_and_splt_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio) # number of rows in test data, rounded to nearest integer\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\n# Split into training and test set with sizes 80% and 20%,respectively\ntrain_set, test_set = shuffle_and_splt_data(housing, 0.2)\n\nVerify training data size\n\nlen(train_set) / len(housing)\n\n0.8\n\n\nVerify test data size\n\nlen(test_set) / len(housing)\n\n0.2\n\n\nAdditionally you can use sklearn to create train and test split, supplying the random seed as an argument\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=RANDOM_SEED)\n\nVerify training data size\n\nlen(train_set) / len(housing)\n\n0.8\n\n\nVerify test data size\n\nlen(test_set) / len(housing)\n\n0.2\n\n\nIn order to ensure consistent entries in test set across data refreshes, you will need to store the indexes that are in the test set.\n\n\n\nSometimes you will want to perform a train/test split using a stratified sample. Here’s a stratified train/test split using income buckets as our strata.\n\nhousing[\"median_income\"].describe()\n\ncount    20640.000000\nmean         3.870671\nstd          1.899822\nmin          0.499900\n25%          2.563400\n50%          3.534800\n75%          4.743250\nmax         15.000100\nName: median_income, dtype: float64\n\n\nCreate income bucket column in dataframe\n\nhousing[\"income_bucket\"] = pd.cut(\n    housing[\"median_income\"],\n    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n    labels=[1, 2, 3, 4, 5],\n)\n\nhousing[\"income_bucket\"].value_counts().sort_index().plot.bar(\n    rot=0, # rotates x-axis labels\n    grid=True # add gridlines\n)\nplt.xlabel(\"Income Bucket\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n\n\n\n\n\n\n\n\nUse sklearn to perform a stratified train/test split 10 times\n\nsplitter = StratifiedShuffleSplit(n_splits=10, test_size = 0.2, random_state=RANDOM_SEED)\nstratified_splits = []\nstratified_split_indices = splitter.split(housing, housing[\"income_bucket\"])\nfor train_index, test_index in stratified_split_indices:\n    stratified_train_set_idx = housing.iloc[train_index]\n    stratified_test_set_idx = housing.iloc[test_index]\n    stratified_splits.append([stratified_train_set_idx, stratified_test_set_idx])\n\nIf you wish to use a single straified test/train split, you can simply use train_test_split()\n\nstrat_train_set, strat_test_set = train_test_split(\n    housing,\n    test_size=0.2,\n    stratify=housing[\"income_bucket\"],\n    random_state=RANDOM_SEED,\n)\n\nCertify the straification resulted in a more balanced test set for income:\n\ndef income_bucket_proportions(data):\n    return data[\"income_bucket\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(\n    housing,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\nproportion_comparison = pd.DataFrame({\n    \"Overall %\": income_bucket_proportions(housing),\n    \"Stratified %\": income_bucket_proportions(strat_test_set),\n    \"Random %\": income_bucket_proportions(test_set),\n})\nproportion_comparison.index.name = \"Income Bucket\"\nproportion_comparison[\"Stratified Error %\"] = proportion_comparison[\"Stratified %\"] / proportion_comparison[\"Overall %\"] - 1\nproportion_comparison[\"Random Error %\"] = proportion_comparison[\"Random %\"] / proportion_comparison[\"Overall %\"] - 1\n\n(proportion_comparison * 100).round(2)\n\n\n\n\n\n\n\n\n\nOverall %\nStratified %\nRandom %\nStratified Error %\nRandom Error %\n\n\nIncome Bucket\n\n\n\n\n\n\n\n\n\n3\n35.06\n35.05\n34.52\n-0.01\n-1.53\n\n\n2\n31.88\n31.88\n30.74\n-0.02\n-3.59\n\n\n4\n17.63\n17.64\n18.41\n0.03\n4.42\n\n\n5\n11.44\n11.43\n12.09\n-0.08\n5.63\n\n\n1\n3.98\n4.00\n4.24\n0.36\n6.45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a scatter plot of the latitudes and longitudes tells us where these district are located in relation to one another.\n\nhousing.plot(\"longitude\", \"latitude\", \"scatter\")\nplt.show()\n\n\n\n\n\n\n\n\nWe observe that our data spans California. Applying an alpha to the plot will better show the districts’ density.\n\nhousing.plot(\"longitude\", \"latitude\", \"scatter\", alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nAdditionally, we can plot these points and add layers for population and median housing price\n\nhousing.plot(\n    kind=\"scatter\",\n    x=\"longitude\",\n    y=\"latitude\",\n    grid=True,\n    s=housing[\"population\"] / 100, # size of points\n    c=\"median_house_value\", # color of points\n    cmap=\"viridis\", # colormap to use for color layer\n    colorbar=True,\n    alpha=0.5,\n    legend=True,\n    figsize=(10,7)\n)\nplt.show()\n\n\n\n\n\n\n\n\nThis we are using location data, we can plot this on a map image\n\nfilename = \"california.png\"\nif not (IMAGES_PATH / filename).is_file():\n    img_url_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n    img_url = img_url_root + \"images/end_to_end_project/\" + filename\n    urllib.request.urlretrieve(img_url, IMAGES_PATH / filename)\nhousing.plot(\n    kind=\"scatter\",\n    x=\"longitude\",\n    y=\"latitude\",\n    grid=False,\n    s=housing[\"population\"] / 100, # size of points\n    c=\"median_house_value\", # color of points\n    cmap=\"viridis\", # colormap to use for color layer\n    colorbar=True,\n    alpha=0.5,\n    legend=True,\n    figsize=(10,7)\n)\naxis_limits = -124.55, -113.95, 32.45, 42.05\nplt.axis(axis_limits)\ncalifornia_img = plt.imread(IMAGES_PATH / filename)\nplt.imshow(california_img, extent=axis_limits)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can return a correlation matrix and look at correlations for our target variable.\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nName: median_house_value, dtype: float64\n\n\nAlso, pandas comes with the ability to create scatter plots for all variables of interest.\nWe observe strong correlations among variables concerning house size, number, and count of rooms. We observe weak correlation between housing_median_age and the other variables\n\nscatter_columns = [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\", \"ocean_proximity\"]\npd.plotting.scatter_matrix(housing[scatter_columns], figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can combine columns into new columns for interaction effects.\n\nhousing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\nhousing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n\nWhen we re-run the correlation matrix, we observe that the derived columns can provide bettwe correlations to the target variable\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\nrooms_per_house       0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedrooms_ratio       -0.255880\nName: median_house_value, dtype: float64\n\n\n\n\n\nFunctions are the preferred way to clean data for ML. Functionalizi8ng the data cleaning process allows you to reproduce your results easily and apply the same cleaning across different projects.\n\n\nFirst, we want to remove the target variable from our training set and store it in its own object.\n\nTARGET_VARIABLE = \"median_house_value\"\nhousing = strat_train_set.drop(TARGET_VARIABLE, axis=1) # drops column\nhousing_labels = strat_test_set[TARGET_VARIABLE].copy()\n\n\n\n\nTO handle missing data, there are three options: - Remove the rows with missing values (pd.DataFrame.dropna()) - Remove attributes with missingness (pd.DataFrame.drop()) - Impute the missing values\nImputation is generally preferred, so as to avoid losing information. We can use scikit-learn for imputation\n\n# available strategies: 'mean', 'median', 'most_frequent', 'constant' (using provided 'fill_value'), or Callable\nimputer = SimpleImputer(strategy=\"median\")\nhousing_numeric_columns = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_numeric_columns)\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SimpleImputer?Documentation for SimpleImputeriFittedSimpleImputer(strategy='median') \n\n\nThe imputer calcualtes the specified statistics and stores them in the statistics_ attribute.\n\nimputer.statistics_\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\n\nhousing_numeric_columns.median().values\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\nTo apply the “fitted” imputer to the data, use the transform method.\n\nX = imputer.transform(housing_numeric_columns)"
  },
  {
    "objectID": "hands_on_machine_learning.html#chapter-2---end-to-end-machine-learning-project",
    "href": "hands_on_machine_learning.html#chapter-2---end-to-end-machine-learning-project",
    "title": "Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow",
    "section": "",
    "text": "Load packages\n\nimport os\nfrom pathlib import Path\nimport tarfile\nimport urllib.request\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n\n\nCode to save and format figures as high-res PNGs\n\nIMAGES_PATH = Path() / \"images\" / \"end_to_end_project\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\n# matplotlib settings\nplt.rc(\"font\", size=14)\nplt.rc(\"axes\", labelsize=14, titlesize=14)\nplt.rc(\"legend\", fontsize=14)\nplt.rc(\"xtick\", labelsize=10)\nplt.rc(\"ytick\", labelsize=10)\n\n\n\n\nSet seed for reproducbility\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED) # set seed for reproducibility\n\nAdditionally, you must set the environment variable PYTHONHASHSEED to \"0\" before python starts.\n\n\n\n\nExtract and load housing data\n\ndef extract_housing_data():\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True) # create new dir if does not exist\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n        with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path = \"datasets\")\n\n\ndef load_housing_data():\n    housing_csv_path = Path(\"datasets/housing/housing.csv\")\n    if not housing_csv_path.is_file():\n        extract_housing_data()\n    return pd.read_csv(housing_csv_path)\n\n\nhousing = load_housing_data()\n\nView housing data structure\n\nhousing.head()\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\n\nhousing.hist(bins=50, figsize=(12,8))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef shuffle_and_splt_data(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio) # number of rows in test data, rounded to nearest integer\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\n# Split into training and test set with sizes 80% and 20%,respectively\ntrain_set, test_set = shuffle_and_splt_data(housing, 0.2)\n\nVerify training data size\n\nlen(train_set) / len(housing)\n\n0.8\n\n\nVerify test data size\n\nlen(test_set) / len(housing)\n\n0.2\n\n\nAdditionally you can use sklearn to create train and test split, supplying the random seed as an argument\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=RANDOM_SEED)\n\nVerify training data size\n\nlen(train_set) / len(housing)\n\n0.8\n\n\nVerify test data size\n\nlen(test_set) / len(housing)\n\n0.2\n\n\nIn order to ensure consistent entries in test set across data refreshes, you will need to store the indexes that are in the test set.\n\n\n\nSometimes you will want to perform a train/test split using a stratified sample. Here’s a stratified train/test split using income buckets as our strata.\n\nhousing[\"median_income\"].describe()\n\ncount    20640.000000\nmean         3.870671\nstd          1.899822\nmin          0.499900\n25%          2.563400\n50%          3.534800\n75%          4.743250\nmax         15.000100\nName: median_income, dtype: float64\n\n\nCreate income bucket column in dataframe\n\nhousing[\"income_bucket\"] = pd.cut(\n    housing[\"median_income\"],\n    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n    labels=[1, 2, 3, 4, 5],\n)\n\nhousing[\"income_bucket\"].value_counts().sort_index().plot.bar(\n    rot=0, # rotates x-axis labels\n    grid=True # add gridlines\n)\nplt.xlabel(\"Income Bucket\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n\n\n\n\n\n\n\n\nUse sklearn to perform a stratified train/test split 10 times\n\nsplitter = StratifiedShuffleSplit(n_splits=10, test_size = 0.2, random_state=RANDOM_SEED)\nstratified_splits = []\nstratified_split_indices = splitter.split(housing, housing[\"income_bucket\"])\nfor train_index, test_index in stratified_split_indices:\n    stratified_train_set_idx = housing.iloc[train_index]\n    stratified_test_set_idx = housing.iloc[test_index]\n    stratified_splits.append([stratified_train_set_idx, stratified_test_set_idx])\n\nIf you wish to use a single straified test/train split, you can simply use train_test_split()\n\nstrat_train_set, strat_test_set = train_test_split(\n    housing,\n    test_size=0.2,\n    stratify=housing[\"income_bucket\"],\n    random_state=RANDOM_SEED,\n)\n\nCertify the straification resulted in a more balanced test set for income:\n\ndef income_bucket_proportions(data):\n    return data[\"income_bucket\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(\n    housing,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\nproportion_comparison = pd.DataFrame({\n    \"Overall %\": income_bucket_proportions(housing),\n    \"Stratified %\": income_bucket_proportions(strat_test_set),\n    \"Random %\": income_bucket_proportions(test_set),\n})\nproportion_comparison.index.name = \"Income Bucket\"\nproportion_comparison[\"Stratified Error %\"] = proportion_comparison[\"Stratified %\"] / proportion_comparison[\"Overall %\"] - 1\nproportion_comparison[\"Random Error %\"] = proportion_comparison[\"Random %\"] / proportion_comparison[\"Overall %\"] - 1\n\n(proportion_comparison * 100).round(2)\n\n\n\n\n\n\n\n\n\nOverall %\nStratified %\nRandom %\nStratified Error %\nRandom Error %\n\n\nIncome Bucket\n\n\n\n\n\n\n\n\n\n3\n35.06\n35.05\n34.52\n-0.01\n-1.53\n\n\n2\n31.88\n31.88\n30.74\n-0.02\n-3.59\n\n\n4\n17.63\n17.64\n18.41\n0.03\n4.42\n\n\n5\n11.44\n11.43\n12.09\n-0.08\n5.63\n\n\n1\n3.98\n4.00\n4.24\n0.36\n6.45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a scatter plot of the latitudes and longitudes tells us where these district are located in relation to one another.\n\nhousing.plot(\"longitude\", \"latitude\", \"scatter\")\nplt.show()\n\n\n\n\n\n\n\n\nWe observe that our data spans California. Applying an alpha to the plot will better show the districts’ density.\n\nhousing.plot(\"longitude\", \"latitude\", \"scatter\", alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nAdditionally, we can plot these points and add layers for population and median housing price\n\nhousing.plot(\n    kind=\"scatter\",\n    x=\"longitude\",\n    y=\"latitude\",\n    grid=True,\n    s=housing[\"population\"] / 100, # size of points\n    c=\"median_house_value\", # color of points\n    cmap=\"viridis\", # colormap to use for color layer\n    colorbar=True,\n    alpha=0.5,\n    legend=True,\n    figsize=(10,7)\n)\nplt.show()\n\n\n\n\n\n\n\n\nThis we are using location data, we can plot this on a map image\n\nfilename = \"california.png\"\nif not (IMAGES_PATH / filename).is_file():\n    img_url_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n    img_url = img_url_root + \"images/end_to_end_project/\" + filename\n    urllib.request.urlretrieve(img_url, IMAGES_PATH / filename)\nhousing.plot(\n    kind=\"scatter\",\n    x=\"longitude\",\n    y=\"latitude\",\n    grid=False,\n    s=housing[\"population\"] / 100, # size of points\n    c=\"median_house_value\", # color of points\n    cmap=\"viridis\", # colormap to use for color layer\n    colorbar=True,\n    alpha=0.5,\n    legend=True,\n    figsize=(10,7)\n)\naxis_limits = -124.55, -113.95, 32.45, 42.05\nplt.axis(axis_limits)\ncalifornia_img = plt.imread(IMAGES_PATH / filename)\nplt.imshow(california_img, extent=axis_limits)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can return a correlation matrix and look at correlations for our target variable.\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nName: median_house_value, dtype: float64\n\n\nAlso, pandas comes with the ability to create scatter plots for all variables of interest.\nWe observe strong correlations among variables concerning house size, number, and count of rooms. We observe weak correlation between housing_median_age and the other variables\n\nscatter_columns = [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\", \"ocean_proximity\"]\npd.plotting.scatter_matrix(housing[scatter_columns], figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can combine columns into new columns for interaction effects.\n\nhousing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\nhousing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n\nWhen we re-run the correlation matrix, we observe that the derived columns can provide bettwe correlations to the target variable\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\nrooms_per_house       0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedrooms_ratio       -0.255880\nName: median_house_value, dtype: float64\n\n\n\n\n\nFunctions are the preferred way to clean data for ML. Functionalizi8ng the data cleaning process allows you to reproduce your results easily and apply the same cleaning across different projects.\n\n\nFirst, we want to remove the target variable from our training set and store it in its own object.\n\nTARGET_VARIABLE = \"median_house_value\"\nhousing = strat_train_set.drop(TARGET_VARIABLE, axis=1) # drops column\nhousing_labels = strat_test_set[TARGET_VARIABLE].copy()\n\n\n\n\nTO handle missing data, there are three options: - Remove the rows with missing values (pd.DataFrame.dropna()) - Remove attributes with missingness (pd.DataFrame.drop()) - Impute the missing values\nImputation is generally preferred, so as to avoid losing information. We can use scikit-learn for imputation\n\n# available strategies: 'mean', 'median', 'most_frequent', 'constant' (using provided 'fill_value'), or Callable\nimputer = SimpleImputer(strategy=\"median\")\nhousing_numeric_columns = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_numeric_columns)\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SimpleImputer?Documentation for SimpleImputeriFittedSimpleImputer(strategy='median') \n\n\nThe imputer calcualtes the specified statistics and stores them in the statistics_ attribute.\n\nimputer.statistics_\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\n\nhousing_numeric_columns.median().values\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\nTo apply the “fitted” imputer to the data, use the transform method.\n\nX = imputer.transform(housing_numeric_columns)"
  }
]